checkpoint_path: /Users/platypus/Desktop/OCL_TS/fsnet/checkpoints_er/

data:
  data: ECL
  root_path: /Users/platypus/Desktop/OCL_TS/fsnet/data/ECL
  freq: h
  features: M     # maybe this indicates if we want to use multiple features or not for input and output
  seq_len: 60
  label_len: 0
  pred_len: 24
  batch_size: 32
  inverse: false
  cols: null
  perc_warm_up: 0.20
  perc_val: 0.05
  scale: true   # data normalization
  timeenc: 2  # differences in computing temporal information in timefeatures.py

run:
  num_workers: 0
  gpu: 0
  use_gpu: false
  use_multi_gpu: false
  devices: "0,1,2,3"

model:    # values as in the original paper
  #enc_in: 12 ------- set in the main based on the dataset
  #c_out: 12 ------ set in the main based on the dataset
  #dec_in: 12 ------- set in the main based on the dataset
  output_dims: 320  # standard ts2vec backbone value
  hidden_dims: 64   # standard ts2vec backbone value
  depth: 10
  regressor_dims: 320

warm_up:
  opt: adam
  learning_rate: 0.001
  patience: 3
  train_epochs: 6
  lradj: type1  # learning rate adjustment (for the schedule)

#online:
#  test_bsz: 1
#  online_learning: full
#  method: er_clever
#  learning_rate: 0.001

tuning:
  n_inner: 1  # number of inner loop updates
  num_samples: 100  # number of tried configurations
  max_concurrent_trials: 5
  #buffer_size: {choice: [500]}
  #learning_rate: {loguniform: [0.00001, 0.01]} #{choice: [0.0001]}
  #n_replay: {choice: [2, 4, 8, 16, 32]} #{choice: [8]} 
  #er_loss_weight: {loguniform: [0.01, 10]} #{choice: [2]}
  #buffer_type: {choice: [fifo, reservoir]} #{choice: [reservoir]}
  buffer_size: 500
  learning_rate: 0.001
  n_replay: 8
  er_loss_weight: 1
  buffer_type: "reservoir"

finetune: false
finetune_model_seed: 0

itr: 1
gamma: 0.9